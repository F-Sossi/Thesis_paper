% Chapter 3: Methodology
% CLAUDE EDIT: Comprehensive methodology chapter 2025-12-07

This chapter describes our methodology for cross-detector descriptor fusion, including the spatial intersection algorithm, scale-matching strategy, and evaluation framework.

\section{DescriptorWorkbench Framework}

We developed DescriptorWorkbench, an open-source evaluation framework for local feature descriptor research. The framework provides:

\begin{itemize}
    \item \textbf{Modular architecture}: Pluggable descriptor extractors, pooling strategies, and matchers
    \item \textbf{Database storage}: SQLite-based experiment tracking with comprehensive metrics
    \item \textbf{YAML configuration}: Declarative experiment specification
    \item \textbf{CLI tools}: \texttt{experiment\_runner} for evaluation, \texttt{keypoint\_manager} for keypoint set operations
\end{itemize}

\subsection{Supported Descriptors}

The framework implements the following descriptor types, utilizing a hybrid architecture of OpenCV for traditional methods and LibTorch for deep learning models:

\begin{table}[h]
\centering
\caption{Descriptor implementations in DescriptorWorkbench}
\label{tab:descriptors}
\begin{tabular}{llrl}
\toprule
\textbf{Type} & \textbf{Family} & \textbf{Dim} & \textbf{Backend} \\
\midrule
SIFT & Traditional & 128 & OpenCV \\
RootSIFT & Traditional & 128 & OpenCV + transform \\
DSP-SIFT v2 & Traditional & 128 & Custom (pyramid-aware) \\
RGBSIFT & Traditional & 384 & Custom (per-channel) \\
SURF & Traditional & 64/128 & OpenCV \\
HardNet & Learned & 128 & LibTorch (C++) \\
SOSNet & Learned & 128 & LibTorch (C++) \\
L2-Net & Learned & 128 & LibTorch (C++) \\
Composite & Fusion & varies & Custom \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Database Schema}

Experiment results are stored in SQLite with the following key tables:

\begin{itemize}
    \item \texttt{experiments}: Descriptor type, keypoint set, parameters
    \item \texttt{results}: mAP, HP-V/HP-I breakdown, verification/retrieval metrics
    \item \texttt{keypoint\_sets}: Named keypoint collections with metadata
    \item \texttt{keypoints}: Individual keypoint records with coordinates and scale
\end{itemize}

\subsection{Deep Learning Integration}

A significant engineering challenge in this work was integrating research-grade deep learning models (typically implemented in Python/PyTorch) into a high-performance C++ evaluation pipeline to ensure fair comparison with optimized traditional descriptors like SIFT.

\subsubsection{Hybrid Architecture}

We developed a hybrid pipeline that leverages the best tools for each stage:

\begin{itemize}
    \item \textbf{Keypoint Detection (Python/Kornia)}: KeyNet detection is performed offline using Python scripts that interface with the Kornia library. This ensures 100\% fidelity to the reference implementation and avoids reimplementation errors. Coordinates are serialized to the SQLite database for reuse.
    \item \textbf{Descriptor Extraction (C++/LibTorch)}: For descriptor extraction, which must occur in the inner loop of matching experiments, we integrated the LibTorch (PyTorch C++) frontend directly into our application.
\end{itemize}

\subsubsection{Rejection of ONNX}

Initially, we attempted to deploy models using the ONNX (Open Neural Network Exchange) format run via OpenCV's DNN module. However, this approach proved unsuitable for our needs:
\begin{itemize}
    \item \textbf{Operator Incompatibility}: Key models like HardNet use specific normalization layers (e.g., \texttt{InstanceNorm}) and control structures that were not fully supported by the OpenCV ONNX importer at the time of development.
    \item \textbf{Performance Stability}: We observed inconsistent behavior and occasional crashes with complex graphs exported from newer PyTorch versions.
\end{itemize}

Consequently, we deprecated the ONNX pipeline in favor of TorchScript. Models are exported to the \texttt{.pt} format and loaded via \texttt{torch::jit::load} within our C++ \texttt{LibTorchWrapper}. This provides native PyTorch execution speed and guarantees numerical equivalence to the Python training code, while operating within the memory space of our C++ benchmarking tool.

\section{Spatial Intersection Algorithm}
\label{sec:intersection_algorithm}

To enable cross-detector descriptor fusion, we need to establish correspondence between keypoints detected by different methods. We employ a mutual nearest neighbor (MNN) algorithm with spatial tolerance.

\subsection{Algorithm Description}

Given two keypoint sets $K_A$ (e.g., SIFT-detected) and $K_B$ (e.g., KeyNet-detected), we compute the intersection as follows:

\begin{enumerate}
    \item Build KD-tree spatial indices for both sets: $T_A$, $T_B$

    \item For each keypoint $k_a \in K_A$:
    \begin{enumerate}
        \item Find nearest neighbor $k_b = \text{NN}(k_a, T_B)$
        \item Check forward tolerance: $\|k_a - k_b\|_2 \leq \tau$
        \item Find reverse nearest neighbor $k_a' = \text{NN}(k_b, T_A)$
        \item Check mutual agreement: $k_a' = k_a$
        \item Check reverse tolerance: $\|k_b - k_a'\|_2 \leq \tau$
        \item Check uniqueness: $k_b$ not already matched
        \item If all checks pass, add $(k_a, k_b)$ to intersection
    \end{enumerate}

    \item Output: Paired keypoint sets $(K_A^*, K_B^*)$ with 1-to-1 correspondence
\end{enumerate}

\subsection{Algorithm Properties}

The MNN algorithm guarantees several desirable properties:

\begin{itemize}
    \item \textbf{1-to-1 correspondence}: Each keypoint matched at most once
    \item \textbf{Symmetry}: Same result regardless of which set is processed first
    \item \textbf{Tolerance-based}: Configurable spatial acceptance threshold
    \item \textbf{Mutual agreement}: Both keypoints must be each other's nearest neighbor
\end{itemize}

\subsection{Tolerance Selection}

Following Mikolajczyk and Schmid's detector evaluation methodology, we use a default tolerance of $\tau = 3.0$ pixels. This value balances:

\begin{itemize}
    \item \textbf{Strict enough}: Prevents misaligned correspondences
    \item \textbf{Loose enough}: Accounts for typical detector variance (1-3 pixels)
    \item \textbf{Literature-supported}: Standard practice in feature matching research
\end{itemize}

\subsection{Implementation}

The intersection algorithm is implemented in the \texttt{keypoint\_manager} CLI tool:

\begin{verbatim}
./keypoint_manager build-intersection \
    --source-a sift_8000 \
    --source-b keynet_8000 \
    --out-a sift_intersection \
    --out-b keynet_intersection \
    --tolerance 3.0
\end{verbatim}

The resulting keypoint sets are stored in the database with full provenance tracking.

\section{Scale-Matching Strategy}
\label{sec:scale_matching}

We observed that SIFT and KeyNet detectors produce keypoints with dramatically different scale distributions. This motivated our scale-matching strategy.

\subsection{Scale Distribution Analysis}

Table~\ref{tab:scale_distribution} shows the scale characteristics of different keypoint sets:

\begin{table}[h]
\centering
\caption{Scale distribution of keypoint sets}
\label{tab:scale_distribution}
\begin{tabular}{lrrr}
\toprule
\textbf{Keypoint Set} & \textbf{Count} & \textbf{Mean Scale} & \textbf{Std Dev} \\
\midrule
sift\_8000 (full) & 2.5M & 4.45px & 3.2px \\
keynet\_8000 (full) & 2.8M & 49.83px & 28.1px \\
sift\_scale\_6px (filtered) & 645K & 10.03px & 4.1px \\
keynet\_scale\_6px (filtered) & 645K & 92.39px & 31.2px \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Scale Filtering Methodology}

To create scale-controlled keypoint sets, we:

\begin{enumerate}
    \item Sort keypoints by scale (descending)
    \item Retain top $k$\% (typically 25\%)
    \item Apply minimum scale threshold (6 pixels) to exclude aliased features
\end{enumerate}

\subsection{Scale-Matched Intersection}

For cross-detector fusion, we combine scale filtering with spatial intersection:

\begin{enumerate}
    \item Generate scale-controlled SIFT keypoints: $K_A^{\text{scale}}$
    \item Generate scale-controlled KeyNet keypoints: $K_B^{\text{scale}}$
    \item Compute spatial intersection: $(K_A^*, K_B^*)$
\end{enumerate}

This produces aligned keypoint pairs where:
\begin{itemize}
    \item SIFT keypoints: Average 7.64 pixel scale
    \item KeyNet keypoints: Average 89.74 pixel scale
    \item Both sets: 111K paired keypoints with 1-to-1 correspondence
\end{itemize}

\section{Descriptor Fusion Methods}

With aligned keypoint sets, we implement two fusion strategies:

\subsection{Weighted Averaging}

\begin{equation}
    d_{\text{avg}} = \alpha \cdot d_A + (1 - \alpha) \cdot d_B
\end{equation}

where $\alpha \in [0, 1]$ controls the contribution of each descriptor. We use $\alpha = 0.5$ for equal weighting.

\textbf{Requirements}:
\begin{itemize}
    \item Same dimensionality: $\text{dim}(d_A) = \text{dim}(d_B)$
    \item Normalization: Both descriptors should be L2-normalized before averaging
\end{itemize}

\subsection{Concatenation}

\begin{equation}
    d_{\text{concat}} = [d_A, d_B]
\end{equation}

\textbf{Properties}:
\begin{itemize}
    \item Preserves both representations fully
    \item Doubles dimensionality: 128D + 128D = 256D
    \item No information loss from aggregation
\end{itemize}

\section{Experimental Pipeline}

Our experimental workflow integrates the components described above into a cohesive pipeline:

\begin{enumerate}
    \item \textbf{Detection}: Generate keypoints for all HPatches images using detectors (SIFT, KeyNet) via Python/Kornia scripts.
    \item \textbf{Intersection}: Compute spatially aligned keypoint subsets using the MNN algorithm ($\tau=3.0$px) and scale filtering.
    \item \textbf{Extraction}: Compute descriptors for these locked keypoints. SIFT/RootSIFT use OpenCV; HardNet/SOSNet use the LibTorch C++ wrapper.
    \item \textbf{Fusion}: (Optional) Combine descriptors via concatenation or averaging.
    \item \textbf{Matching \& Evaluation}: Perform retrieval and matching tasks, computing metrics against ground-truth homographies.
\end{enumerate}

\section{Evaluation Methodology}

We employ a multi-faceted evaluation strategy based on the protocols defined by Balntas et al. and expanded by Bojani\'{c} et al.

\subsection{Task 1: Image Matching (mAP)}

The primary metric for descriptor utility is Mean Average Precision (mAP) in an image matching context.

\textbf{Protocol}:
\begin{enumerate}
    \item For each image pair (reference $I_A$, target $I_B$):
    \item Match descriptors using the Second Nearest Neighbor (SNN) ratio test.
    \item A match $(d_A^i, d_B^j)$ is correct if the geometric projection error $||H \cdot k_A^i - k_B^j||_2 \leq \tau$.
    \item Compute Average Precision (AP) as the area under the Precision-Recall curve.
\end{enumerate}

We report \textbf{True Micro mAP}: An Information Retrieval (IR) style metric where we aggregate AP over all queries. Crucially, we enforce a \textbf{Single Ground Truth (R=1)} policy: for a given keypoint in $I_A$, there is at most one correct match in $I_B$ (the nearest neighbor in the intersection set).

\subsection{Task 2: Keypoint Verification}

Following Bojani\'{c} et al., verification tests the descriptor's ability to distinguish true correspondences from false ones.

\textbf{Binary Classification}:
\begin{itemize}
    \item \textbf{Positive pairs}: Spatially corresponding keypoints from sequence pairs.
    \item \textbf{Negative pairs}: Spatially non-corresponding keypoints (distractors).
\end{itemize}
We compute the Area Under the ROC Curve (AUC) to measure discriminative power independent of the nearest-neighbor search strategy.

\subsection{Task 3: Keypoint Retrieval}

This task evaluates the descriptor's ranking capability. For a query patch, the system must rank a database of target patches.

\textbf{Labels}:
\begin{itemize}
    \item $y=1$: The true geometric match.
    \item $y=0$: "Hard negatives" (patches from the same image but different locations).
    \item $y=-1$: "Easy negatives" (patches from different sequences/scenes).
\end{itemize}
This metric highlights whether a descriptor has learned semantically meaningful representations that separate true matches from visually similar but geometrically distinct structures.

\subsection{Aggregation and Breakdown}

To account for the diversity of the HPatches dataset, we report results in two aggregation modes:
\begin{itemize}
    \item \textbf{Micro Average}: Aggregates all queries globally. biases towards scenes with more keypoints.
    \item \textbf{Macro Average}: Computes metrics per scene, then averages across scenes. This ensures that texture-poor scenes (with fewer keypoints) contribute equally to the final score.
\end{itemize}

Results are further stratified by scene type:
\begin{itemize}
    \item \textbf{HP-V (Viewpoint)}: 59 sequences with significant geometric deformations.
    \item \textbf{HP-I (Illumination)}: 57 sequences with lighting changes (day/night, flash/no-flash).
\end{itemize}

\section{Experimental Design}

\subsection{Independent Variables}

Our experiments vary the following factors:

\begin{itemize}
    \item \textbf{Descriptor type}: SIFT, RootSIFT, DSP-SIFT, HardNet, SOSNet
    \item \textbf{Keypoint set}: Full, scale-controlled, intersection, scale-matched intersection
    \item \textbf{Fusion method}: None, averaging, concatenation
    \item \textbf{Fusion pairs}: SIFT+CNN, CNN+CNN
\end{itemize}

\subsection{Dependent Variables}

We measure:
\begin{itemize}
    \item Mean Average Precision (mAP) - primary metric
    \item HP-V and HP-I breakdown
    \item Keypoint verification AP (when enabled)
    \item Keypoint retrieval AP (when enabled)
\end{itemize}

\subsection{Experiment Execution}

Experiments are specified in YAML configuration files and executed via:

\begin{verbatim}
./experiment_runner config/experiments/experiment.yaml
\end{verbatim}

Results are automatically stored in the SQLite database with full parameter logging for reproducibility.
