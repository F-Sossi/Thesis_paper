% Chapter 2: Background and Related Work
% CLAUDE EDIT: New background chapter 2025-12-07

This chapter reviews the foundations of local feature detection and description, covering traditional and learned approaches, descriptor fusion methods, and evaluation benchmarks.

\section{Local Feature Detection}

\subsection{Traditional Detectors}

The SIFT (Scale-Invariant Feature Transform) detector identifies keypoints by searching for scale-space extrema in a Difference-of-Gaussian (DoG) pyramid. Key characteristics include:

\begin{itemize}
    \item \textbf{Scale invariance}: Keypoints are detected across multiple octaves, with scale encoded in the keypoint metadata
    \item \textbf{Orientation assignment}: Dominant gradient orientation is computed for rotation invariance
    \item \textbf{Sub-pixel localization}: Taylor expansion refines keypoint position
\end{itemize}

Other traditional detectors include Harris corners, FAST, and SURF, each with different trade-offs between repeatability and computational cost.

\subsection{Learned Detectors}

KeyNet represents a hybrid approach combining hand-crafted and learned filters:

\begin{itemize}
    \item \textbf{Handcrafted filters}: Provide anchor structure for stable detection
    \item \textbf{Learned filters}: Trained to localize, score, and rank repeatable features
    \item \textbf{Multi-scale}: Operates on image pyramids similar to SIFT
\end{itemize}

KeyNet was designed specifically for pairing with learned descriptors like HardNet and SOSNet, achieving state-of-the-art repeatability on HPatches.

\subsection{End-to-End Learned Pipelines}

While KeyNet focuses on detection to be paired with separate descriptors, other approaches integrate detection and description into a single end-to-end trainable network:

\begin{itemize}
    \item \textbf{SuperPoint} (DeTone et al., 2018): A fully convolutional network trained using self-supervision. It employs a single shared encoder and two separate decoder heads for interest point detection and descriptor generation.
    \item \textbf{LF-Net} (Ono et al., 2018): Learns local features without human supervision by optimizing a keypoint correspondence objective, enforcing geometry constraints across multi-view images.
    \item \textbf{ALIKE} (Zhao et al., 2023): A lightweight framework that combines the strengths of handcrafted and learned methods, using a deformable transformation module to improve geometric invariance while maintaining high computational efficiency.
\end{itemize}

These end-to-end methods offer a compelling alternative to the detect-then-describe pipeline, though they often require strictly coupled detector-descriptor pairs, unlike the mix-and-match flexibility we investigate with KeyNet and SIFT.

\subsection{Detector Characteristics}

A critical observation for this thesis is that different detectors produce keypoints with different scale distributions:

\begin{itemize}
    \item \textbf{SIFT detector}: Produces predominantly small-scale keypoints (average 4.45 pixels in our experiments)
    \item \textbf{KeyNet detector}: Produces larger-scale keypoints (average 49.83 pixels), approximately 10$\times$ larger
\end{itemize}

This scale difference has significant implications for descriptor quality, as larger patches contain more distinctive information.

\section{Local Feature Descriptors}

\subsection{Traditional Descriptors}

\subsubsection{SIFT Descriptor}

The SIFT descriptor computes a 128-dimensional vector from a 16$\times$16 pixel patch:

\begin{enumerate}
    \item Divide patch into 4$\times$4 grid of cells
    \item Compute 8-bin gradient orientation histogram per cell
    \item Concatenate to form 4$\times$4$\times$8 = 128 dimensions
    \item L2 normalize, clip values $>$ 0.2, re-normalize
\end{enumerate}

The resulting descriptor is \textit{non-negative} with values typically in [0, 0.3].

\subsubsection{Domain-Size Pooling (DSP-SIFT)}

Dong and Soatto introduced Domain-Size Pooling (DSP), which aggregates SIFT descriptors computed at multiple scales around each keypoint:

\begin{equation}
    d_{\text{DSP}} = \frac{1}{N} \sum_{i=1}^{N} d_{\sigma_i}
\end{equation}

where $d_{\sigma_i}$ is the SIFT descriptor computed at scale $\sigma_i$. DSP improves matching accuracy by capturing multi-scale information.

\subsubsection{RootSIFT}

RootSIFT applies element-wise square root after L1 normalization, which is equivalent to using the Hellinger kernel:

\begin{equation}
    d_{\text{RootSIFT}} = \sqrt{d_{\text{SIFT}} / \|d_{\text{SIFT}}\|_1}
\end{equation}

This transformation improves matching performance, particularly for illumination changes.

\subsection{Learned Descriptors}

\subsubsection{HardNet}

HardNet is trained using hard negative mining with triplet loss:

\begin{equation}
    L = \max(0, m + d(a, p) - d(a, n^-))
\end{equation}

where $(a, p)$ is a matching pair, $n^-$ is the hardest negative in the batch, and $m$ is the margin. The resulting 128-dimensional descriptor is \textit{zero-centered} with values typically in [-0.3, +0.3].

\subsubsection{SOSNet}

SOSNet (Second Order Similarity Network) extends HardNet by incorporating second-order information:

\begin{equation}
    L_{\text{SOS}} = L_{\text{triplet}} + \lambda L_{\text{second-order}}
\end{equation}

SOSNet achieves similar performance to HardNet with slightly better generalization.

\subsubsection{Other Learned Baselines}

In addition to state-of-the-art descriptors like HardNet and SOSNet, we consider earlier learned descriptors to provide a comprehensive baseline:

\begin{itemize}
    \item \textbf{TFeat}: A lightweight descriptor trained with triplet loss using a shallow convolutional network.
    \item \textbf{L2-Net}: A discriminative descriptor that served as a precursor to HardNet, emphasizing L2 distance optimization in the feature space.
\end{itemize}

These descriptors, while older, represent important milestones in the transition from handcrafted to learned features and are included in our ONNX-based evaluation framework.

\subsection{Distribution Characteristics}

A key observation critical for understanding fusion behavior:

\begin{table}[h]
\centering
\caption{Descriptor value distribution characteristics}
\label{tab:distributions}
\begin{tabular}{lrrr}
\toprule
\textbf{Descriptor} & \textbf{Range} & \textbf{Mean} & \textbf{Type} \\
\midrule
SIFT & [0, 0.3] & $\approx$ 0.05 & Non-negative \\
RootSIFT & [0, 0.4] & $\approx$ 0.08 & Non-negative \\
DSP-SIFT & [0, 0.3] & $\approx$ 0.05 & Non-negative \\
HardNet & [-0.3, +0.3] & $\approx$ 0 & Zero-centered \\
SOSNet & [-0.3, +0.3] & $\approx$ 0 & Zero-centered \\
\bottomrule
\end{tabular}
\end{table}

This incompatibility between non-negative and zero-centered distributions is central to understanding fusion failures.

\section{Descriptor Fusion Approaches}

\subsection{Early Fusion (Feature-Level)}

Early fusion combines descriptors before matching:

\textbf{Concatenation:}
\begin{equation}
    d_{\text{concat}} = [d_A, d_B]
\end{equation}

\textbf{Weighted Averaging:}
\begin{equation}
    d_{\text{avg}} = \alpha \cdot d_A + (1 - \alpha) \cdot d_B
\end{equation}

Both approaches require spatial alignment of keypoints when descriptors come from different detectors.

\subsection{Late Fusion (Score-Level)}

Late fusion combines matching scores rather than descriptors:

\begin{equation}
    s_{\text{fused}} = \alpha \cdot s_A + (1 - \alpha) \cdot s_B
\end{equation}

This approach does not require keypoint alignment but cannot create a unified descriptor representation.

\subsection{Research Gap}

Prior work has explored:
\begin{itemize}
    \item Detector-descriptor pairing studies showing mismatch penalties
    \item Late fusion of matching scores (Dempster-Shafer theory)
    \item Ensemble methods in image matching challenges
\end{itemize}

However, \textit{cross-detector early fusion}---averaging or concatenating descriptors from keypoints detected by different methods---remains unexplored. This thesis addresses this gap through our spatial intersection methodology.

\section{Evaluation Benchmarks}

\subsection{HPatches}

The HPatches benchmark provides:
\begin{itemize}
    \item 116 sequences with ground-truth homographies
    \item 59 viewpoint sequences (geometric changes)
    \item 57 illumination sequences (photometric changes)
    \item Standard protocol for descriptor evaluation
\end{itemize}

\subsection{Evaluation Tasks}

Bojanic et al. define three evaluation tasks:

\textbf{Image Matching}: Match descriptors between image pairs using SNN ratio test, compute precision-recall curves, report mean Average Precision (mAP).

\textbf{Keypoint Verification}: Binary classification distinguishing true correspondences from distractors sampled from other sequences.

\textbf{Keypoint Retrieval}: Three-tier ranking task with labels $y \in \{-1, 0, +1\}$ distinguishing true positives, hard negatives (same sequence but not closest), and distractors.

We implement all three tasks in our DescriptorWorkbench framework.
