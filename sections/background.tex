% Chapter 2: Background and Related Work
% DRAFTING: New background chapter 2025-12-07

This chapter reviews the foundations of local feature detection and description, covering traditional and learned approaches, descriptor fusion methods, and evaluation benchmarks.

\section{Local Feature Detection}

\subsection{Traditional Detectors}

The SIFT (Scale-Invariant Feature Transform) detector~\cite{lowe2004distinctive} identifies keypoints by searching for scale-space extrema in a Difference-of-Gaussian (DoG) pyramid. Key characteristics include:

\begin{itemize}
    \item \textbf{Scale invariance}: Keypoints are detected across multiple octaves, with scale encoded in the keypoint metadata
    \item \textbf{Orientation assignment}: Dominant gradient orientation is computed for rotation invariance
    \item \textbf{Sub-pixel localization}: Taylor expansion refines keypoint position
\end{itemize}

Other traditional detectors include Harris corners, FAST, and SURF~\cite{bay2006surf}, each with different trade-offs between repeatability and computational cost.

\subsection{Learned Detectors}

KeyNet~\cite{barroso2019key} represents a hybrid approach combining hand-crafted and learned filters:

\begin{itemize}
    \item \textbf{Handcrafted filters}: Provide anchor structure for stable detection
    \item \textbf{Learned filters}: Trained to localize, score, and rank repeatable features
    \item \textbf{Multi-scale}: Operates on image pyramids similar to SIFT
\end{itemize}

KeyNet was designed specifically for pairing with learned descriptors like HardNet and SOSNet, achieving state-of-the-art repeatability on HPatches.

\subsection{End-to-End Learned Pipelines}

While KeyNet focuses on detection to be paired with separate descriptors, other approaches integrate detection and description into a single end-to-end trainable network:

\begin{itemize}
    \item \textbf{SuperPoint}~\cite{detone2018superpoint}: A fully convolutional network trained using self-supervision. It employs a single shared encoder and two separate decoder heads for interest point detection and descriptor generation.
    \item \textbf{LF-Net}~\cite{ono2018lf}: Learns local features without human supervision by optimizing a keypoint correspondence objective, enforcing geometry constraints across multi-view images.
    \item \textbf{ALIKE}~\cite{zhao2023aliked}: A lightweight framework that combines the strengths of handcrafted and learned methods, using a deformable transformation module to improve geometric invariance while maintaining high computational efficiency.
\end{itemize}

These end-to-end methods offer a compelling alternative to the detect-then-describe pipeline, though they often require strictly coupled detector-descriptor pairs, unlike the mix-and-match flexibility we investigate with KeyNet and SIFT.

\subsection{Detector Characteristics}

A critical observation for this thesis is that different detectors produce keypoints with different scale distributions:

\begin{itemize}
    \item \textbf{SIFT detector}: Produces predominantly small-scale keypoints (average 4.45 pixels in our experiments)
    \item \textbf{KeyNet detector}: Produces larger-scale keypoints (average 49.83 pixels), approximately 10$\times$ larger
\end{itemize}

This scale difference has significant implications for descriptor quality, as larger patches contain more distinctive information.

\section{Local Feature Descriptors}

\subsection{Traditional Descriptors}

\subsubsection{SIFT Descriptor}

The SIFT descriptor~\cite{lowe2004distinctive} computes a 128-dimensional vector from a 16$\times$16 pixel patch:

\begin{enumerate}
    \item Divide patch into 4$\times$4 grid of cells
    \item Compute 8-bin gradient orientation histogram per cell
    \item Concatenate to form 4$\times$4$\times$8 = 128 dimensions
    \item L2 normalize, clip values $>$ 0.2, re-normalize
\end{enumerate}

The resulting descriptor is \textit{non-negative} with values typically in [0, 0.3].

\subsubsection{Domain-Size Pooling (DSP-SIFT)}

Dong and Soatto~\cite{dong2015domain} introduced Domain-Size Pooling (DSP), which aggregates SIFT descriptors computed at multiple scales around each keypoint:

\begin{equation}
    d_{\text{DSP}} = \frac{1}{N} \sum_{i=1}^{N} d_{\sigma_i}
\end{equation}

where $d_{\sigma_i}$ is the SIFT descriptor computed at scale $\sigma_i$. DSP improves matching accuracy by capturing multi-scale information.

\subsubsection{RootSIFT}

RootSIFT applies element-wise square root after L1 normalization, which is equivalent to using the Hellinger kernel:

\begin{equation}
    d_{\text{RootSIFT}} = \sqrt{d_{\text{SIFT}} / \|d_{\text{SIFT}}\|_1}
\end{equation}

This transformation improves matching performance, particularly for illumination changes.

\subsection{Learned Descriptors}

\subsubsection{HardNet}

HardNet~\cite{mishchuk2017working} is trained using hard negative mining with triplet loss:

\begin{equation}
    L = \max(0, m + d(a, p) - d(a, n^-))
\end{equation}

where $(a, p)$ is a matching pair, $n^-$ is the hardest negative in the batch, and $m$ is the margin. The resulting 128-dimensional descriptor is \textit{zero-centered} with values typically in [-0.3, +0.3].

\subsubsection{SOSNet}

SOSNet (Second Order Similarity Network)~\cite{tian2019sosnet} extends HardNet by incorporating second-order information:

\begin{equation}
    L_{\text{SOS}} = L_{\text{triplet}} + \lambda L_{\text{second-order}}
\end{equation}

SOSNet achieves similar performance to HardNet with slightly better generalization.

\subsection{Descriptor Magnitude and Normalization}
% DRAFTING: Updated to reflect magnitude mismatch finding, not distribution incompatibility

A critical consideration for descriptor fusion is \textit{magnitude matching}. Different descriptor families produce values at vastly different scales before normalization:

\begin{table}[h]
\centering
\caption{Descriptor magnitude characteristics (before and after L2 normalization)}
\label{tab:magnitudes}
\begin{tabular}{lrrl}
\toprule
\textbf{Descriptor} & \textbf{Raw Range} & \textbf{After L2 Norm} & \textbf{Notes} \\
\midrule
SIFT & [0, 512] & [0, 0.3] & Gradient histogram counts \\
RootSIFT & [0, 22] & [0, 0.4] & After sqrt transform \\
HoNC & [0, 1] & [0, 0.3] & Normalized color histogram \\
HardNet & [-0.3, +0.3] & [-0.3, +0.3] & Trained with L2 output \\
SOSNet & [-0.3, +0.3] & [-0.3, +0.3] & Trained with L2 output \\
\bottomrule
\end{tabular}
\end{table}

When fusing descriptors from different families, \textbf{magnitude mismatch} can cause one descriptor to dominate distance calculations. For example, raw SIFT values (0--512) would overwhelm HardNet values (--0.3 to +0.3) in a concatenated descriptor. The solution is to L2 normalize each component \textit{before} fusion, ensuring equal contribution regardless of original magnitude.

\section{Descriptor Fusion Approaches}

\subsection{Early Fusion (Feature-Level)}

Early fusion combines descriptors before matching:

\textbf{Concatenation:}
\begin{equation}
    d_{\text{concat}} = [d_A, d_B]
\end{equation}

\textbf{Weighted Averaging:}
\begin{equation}
    d_{\text{avg}} = \alpha \cdot d_A + (1 - \alpha) \cdot d_B
\end{equation}

Both approaches require spatial alignment of keypoints when descriptors come from different detectors.

\subsection{Late Fusion (Score-Level)}

Late fusion combines matching scores rather than descriptors:

\begin{equation}
    s_{\text{fused}} = \alpha \cdot s_A + (1 - \alpha) \cdot s_B
\end{equation}

This approach does not require keypoint alignment but cannot create a unified descriptor representation.

\subsection{Research Gap}

Prior work has explored:
\begin{itemize}
    \item Detector-descriptor pairing studies showing mismatch penalties
    \item Late fusion of matching scores (Dempster-Shafer theory)
    \item Ensemble methods in image matching challenges
\end{itemize}

However, \textit{cross-detector early fusion}---averaging or concatenating descriptors from keypoints detected by different methods---remains unexplored. This thesis addresses this gap through our spatial intersection methodology.

\section{Evaluation Benchmarks}

\subsection{HPatches Dataset}

The HPatches benchmark~\cite{balntas2017hpatches} provides:
\begin{itemize}
    \item 116 sequences with ground-truth homographies
    \item 59 viewpoint sequences (geometric changes)
    \item 57 illumination sequences (photometric changes)
    \item Pre-extracted 65$\times$65 grayscale patches (original benchmark)
    \item Full images for keypoint-based evaluation
\end{itemize}

\subsection{Two Evaluation Protocols}
% DRAFTING: Added to distinguish patch vs full-image task protocols

We employ two distinct evaluation protocols, each with three tasks. The protocols differ in whether keypoint detection is part of the evaluation.

\subsubsection{Original HPatches Patch Protocol (Balntas et al.)}

The original HPatches benchmark~\cite{balntas2017hpatches} evaluates descriptors on \textit{pre-extracted patches}, removing keypoint detection as a variable:

\textbf{Patch Matching}: For each reference patch, rank all target patches from the same sequence by descriptor distance. Report mAP based on whether the correct correspondence ranks first.

\textbf{Patch Verification}: Binary classification of patch pairs as ``same location'' (positive) or ``different location'' (negative). Negatives include both same-sequence and different-sequence patches. Report AP.

\textbf{Patch Retrieval}: Given a query patch, rank a gallery containing true matches and distractors from different sequences. Report mAP.

We use this protocol for our \textbf{color patch benchmark} (Section~\ref{sec:patch_results}) to isolate descriptor fusion effects.

\subsubsection{Bojanic et al. Full-Image Protocol}

Bojanic et al.~\cite{bojanic2020comparison} define an evaluation protocol for \textit{full images with detected keypoints}:

\textbf{Image Matching}: Match descriptors between image pairs using the Second Nearest Neighbor (SNN) ratio test. A match is correct if the geometric reprojection error is below threshold. Report mAP.

\textbf{Keypoint Verification}: Binary classification distinguishing true correspondences from distractors sampled from \textit{other sequences} (not same-sequence negatives).

\textbf{Keypoint Retrieval}: Three-tier ranking with labels $y \in \{-1, 0, +1\}$: true positives (+1), hard negatives from the same sequence (0, ignored in scoring), and distractors from other sequences (-1).

We use this protocol for our \textbf{full-image experiments} (Sections~\ref{sec:baseline_results}--\ref{sec:intersection_results}) to study detector effects.

\subsubsection{Key Differences}

\begin{table}[h]
\centering
\caption{Comparison of evaluation protocols}
\label{tab:protocols}
\begin{tabular}{lll}
\toprule
\textbf{Aspect} & \textbf{Patch Protocol} & \textbf{Bojanic Protocol} \\
\midrule
Input & Pre-extracted patches & Full images \\
Keypoint detection & Not evaluated & Part of pipeline \\
Verification negatives & Same + different sequence & Different sequence only \\
Retrieval labeling & Binary (pos/distractor) & Three-tier ($-1, 0, +1$) \\
Isolates & Descriptor quality & Detector + descriptor \\
\bottomrule
\end{tabular}
\end{table}

We implement both protocols in DescriptorWorkbench to enable controlled experiments.
