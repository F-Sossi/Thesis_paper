% Chapter 4: Experiments and Results

\chapter{Experiments and Results}%
\label{chap:results}
% TODO: Remove DRAFTING comments before final submission
\todo[inline]{Add figures: descriptor\_distributions.png, fusion\_contribution\_analysis.png from analysis/output/}

This chapter presents experimental results from two complementary evaluation pipelines:

\begin{itemize}
    \item \textbf{Full-Image Pipeline} (Sections~\ref{sec:baseline_results}--\ref{sec:intersection_results}): Evaluates detector effects including scale control and spatial intersection. In this pipeline, both keypoint quality and descriptor quality affect results.

    \item \textbf{Patch Benchmark Pipeline} (Section~\ref{sec:patch_results}): Evaluates descriptor fusion with keypoint quality held constant. All descriptors are computed on identical pre-extracted patches, isolating descriptor effects.
\end{itemize}

This separation keeps the claims aligned with the variables: detector consensus findings come from the full-image pipeline, while descriptor fusion findings come from the patch pipeline where descriptors are the only variable.

% ===================================================================
\section{Experimental Setup}
\label{sec:experimental_setup}
% ===================================================================

\subsection{Dataset and Metrics}

We evaluate all experiments on the HPatches benchmark~\cite{balntas2017hpatches}, which consists of 116 image sequences with ground-truth homographies. The dataset is divided into 59 viewpoint sequences (geometric transformations) and 57 illumination sequences (photometric changes).

As defined in Chapter~\ref{chap:methodology}, we primarily report \textbf{Mean Average Precision (mAP)} using the True Micro mAP definition (single ground truth per query). We further breakdown results by sequence type:
\begin{itemize}
    \item \textbf{HP-V}: Viewpoint sequences (measuring geometric invariance)
    \item \textbf{HP-I}: Illumination sequences (measuring photometric invariance)
\end{itemize}

% ===================================================================
\section{Baseline Descriptor Performance}
\label{sec:baseline_results}
% ===================================================================

We first establish baseline performance for traditional and learned descriptors using their native keypoint detectors without any scale filtering.

\begin{table}[ht]
\centering
\caption{Baseline descriptor performance on full keypoint sets (SIFT ~2.5M, KeyNet ~2.8M)}%
\label{tab:baselines}
\begin{tabular}{llrrr}
\toprule
\textbf{Descriptor} & \textbf{Keypoint Set} & \textbf{mAP} & \textbf{HP-V} & \textbf{HP-I} \\
\midrule
SIFT & sift\_8000 & 44.5\% & 45.9\% & 43.1\% \\
RootSIFT & sift\_8000 & 46.7\% & 46.2\% & 47.2\% \\
HardNet & keynet\_8000 & 64.5\% & 63.8\% & 65.3\% \\
SOSNet & keynet\_8000 & 64.3\% & 63.4\% & 65.2\% \\
\bottomrule
\end{tabular}
\end{table}

The learned descriptors (HardNet, SOSNet) outperform SIFT by roughly 20 percentage points of mAP\@. SIFT shows a slight preference for viewpoint changes, while the CNN descriptors perform slightly better on illumination sequences.

% ===================================================================
\section{Impact of Scale Control}%
\label{sec:scale_control}
% ===================================================================

A core hypothesis is that keypoint scale is a dominant factor in matching performance. By filtering the keypoint sets to retain only the largest 25\% of features (Scale-Controlled sets), we observe large performance improvements across all descriptor types.

\begin{table}[ht]
\centering
\caption{Impact of Scale Control (filtering small keypoints)}
\label{tab:scale_control}
\begin{tabular}{llrrr}
\toprule
\textbf{Configuration} & \textbf{Metric} & \textbf{Full Set} & \textbf{Scale Filtered} & \textbf{Improvement} \\
\midrule
\textbf{SIFT} & mAP & 44.5\% & \textbf{62.8\%} & \textbf{+18.3\%} \\
& HP-V & 45.9\% & 65.7\% & +19.8\% \\
& HP-I & 43.1\% & 59.8\% & +16.7\% \\
\midrule
\textbf{HardNet} & mAP & 64.5\% & \textbf{78.1\%} & \textbf{+13.6\%} \\
& HP-V & 63.8\% & 76.9\% & +13.1\% \\
& HP-I & 65.3\% & 79.3\% & +14.0\% \\
\bottomrule
\end{tabular}
\end{table}

Table~\ref{tab:scale_control} shows that removing small, unstable keypoints improves SIFT by over 18 percentage points, bringing it close to the baseline performance of HardNet. For HardNet, scale control yields a 13.6\% gain, pushing it to 78.1\% mAP\@. This indicates that descriptor distinctiveness is strongly correlated with patch size.

% ===================================================================
\section{Impact of Spatial Intersection}%
\label{sec:intersection_results}
% ===================================================================

Our fusion methodology relies on finding the spatial intersection of keypoints detected by SIFT and KeyNet. We analyzed whether this intersection step itself acts as a quality filter.

\begin{table}[ht]
\centering
\caption{Performance of HardNet on different keypoint subsets}%
\label{tab:intersection_analysis}
\begin{tabular}{lrrr}
\toprule
\textbf{Keypoint Set} & \textbf{mAP} & \textbf{HP-V} & \textbf{HP-I} \\
\midrule
Full KeyNet Set & 64.5\% & 63.8\% & 65.3\% \\
Scale-Controlled & 78.1\% & 76.9\% & 79.3\% \\
\textbf{Spatial Intersection} & \textbf{82.1\%} & \textbf{81.5\%} & \textbf{82.7\%} \\
\bottomrule
\end{tabular}
\end{table}

Table~\ref{tab:intersection_analysis} shows that features detected by \textit{both} detectors are higher quality than those detected by KeyNet alone, even after scale filtering. The intersection set yields an additional 4.0\% improvement in mAP, suggesting that detector consensus is a proxy for feature repeatability.

% ===================================================================
\section{Descriptor Fusion Results}
\label{sec:fusion_results}
% ===================================================================

We evaluated two fusion strategies: concatenation and weighted averaging. We tested these on two classes of pairings: Cross-Family (SIFT+CNN) and Intra-Family (CNN+CNN).

\subsection{Cross-Family Fusion on Full Images}
% DRAFTING: Simplified - full-image cross-family fusion not the focus

In the full-image pipeline, cross-family fusion (SIFT+CNN) on intersection keypoints did not yield performance gains over the CNN baseline. The intersection set already selects high-quality keypoints, and adding SIFT to HardNet provides no additional benefit---the information captured by SIFT is already represented in the CNN descriptor.

For controlled evaluation of cross-family fusion, including the effects of magnitude matching and complementary descriptors, see Section~\ref{sec:patch_results} (Color Patch Benchmark Results), where keypoint quality is held constant.

% ============================================================================
% DRAFTING: Added SIFT-family fusion analysis for professor's questions
% ============================================================================
\subsection{Same-Family Fusion (SIFT Variants)}

We also evaluated fusion within the SIFT family to determine if combining grayscale and color descriptors provides benefit.

\begin{table}[ht]
\centering
\caption{SIFT-family fusion results on scale-controlled keypoints}
\label{tab:sift_family_fusion}
\begin{tabular}{llr}
\toprule
\textbf{Configuration} & \textbf{Fusion Method} & \textbf{mAP} \\
\midrule
SIFT alone & --- & 63.86\% \\
DSPSIFT\_V2 alone & --- & 65.31\% \\
DSPRGBSIFT\_V2 alone & --- & 66.03\% \\
\midrule
DSPSIFT + DSPRGBSIFT & Concatenate (256D) & 65.77\% \\
DSPSIFT + DSPRGBSIFT & Average (128D) & 65.37\% \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key finding:} Same-family fusion provides minimal benefit. The best single descriptor (DSPRGBSIFT\_V2 at 66.03\%) slightly outperforms both fusion configurations. This suggests that SIFT-family descriptors, even when one uses color (RGB) and one uses grayscale, capture highly correlated information when computed on the same keypoints.

\subsubsection{SIFT + SURF Fusion on Intersection Sets}

We tested cross-detector fusion using SIFT and SURF descriptors on their spatial intersection:

\begin{table}[ht]
\centering
\caption{SIFT-SURF fusion on intersection keypoints}
\label{tab:sift_surf_fusion}
\begin{tabular}{lr}
\toprule
\textbf{Configuration} & \textbf{mAP} \\
\midrule
DSPSIFT\_V2 on intersection & 74.93\% \\
RGBSIFT on intersection & 75.03\% \\
SURF on intersection & 75.08\% \\
\midrule
SIFT + SURF (concatenate) & 74.37\% \\
\bottomrule
\end{tabular}
\end{table}

Again, fusion slightly underperforms the best single descriptor. The intersection filtering itself provides the primary benefit---once keypoints are filtered to those detected by both SIFT and SURF, individual descriptors already perform well, and fusion adds little value.

\subsubsection{Why Same-Family Fusion Fails to Help}

The lack of fusion benefit can be attributed to three factors:

\begin{enumerate}
    \item \textbf{Correlated information}: SIFT-family descriptors on the same keypoints capture similar gradient histogram information, even when one uses color channels.

    \item \textbf{Quality ceiling}: Scale filtering and intersection already select the highest-quality keypoints, leaving little room for fusion to provide additional signal.

    \item \textbf{Averaging dilutes distinctiveness}: When descriptors are similar, averaging produces a descriptor that is less distinctive than either component, while concatenation doubles dimensionality without adding complementary information.
\end{enumerate}

\textbf{Note on normalization:} All descriptors are L2-normalized per row after pooling, so magnitude differences are not the issue. The fundamental problem is that SIFT-family descriptors provide redundant rather than complementary information.
% ============================================================================
% END DRAFTING
% ============================================================================

\subsection{Intra-Family Fusion (HardNet + SOSNet)}

Fusing two learned descriptors proved highly effective. Table~\ref{tab:fusion_results} shows the results for fusing HardNet and SOSNet on the scale-matched intersection set.

\begin{table}[ht]
\centering
\caption{CNN+CNN Fusion Results (Scale-Matched Intersection)}
\label{tab:fusion_results}
\begin{tabular}{llrrr}
\toprule
\textbf{Descriptor} & \textbf{Fusion} & \textbf{mAP} & \textbf{HP-V} & \textbf{HP-I} \\
\midrule
HardNet & None & 82.1\% & 81.5\% & 82.7\% \\
SOSNet & None & 81.9\% & 81.2\% & 82.5\% \\
HardNet + SOSNet & Weighted Avg & 92.3\% & 91.4\% & 93.2\% \\
\textbf{HardNet + SOSNet} & \textbf{Concat} & \textbf{93.4\%} & \textbf{92.6\%} & \textbf{94.2\%} \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key Finding:} The concatenation of HardNet and SOSNet achieves \textbf{93.4\%} mAP, an 11.3 percentage point improvement over the single best descriptor (HardNet).
\todo[inline]{Verify 93.4\% matches database query and is consistent with abstract/conclusion}

The success of this fusion suggests that while both networks are trained on similar data, they learn complementary representations of the image patches. Concatenation preserves this distinct information, whereas averaging tends to dilute it slightly (92.3\% vs 93.4\%).

% ===================================================================
\section{Viewpoint vs. Illumination Analysis}
\label{sec:viewpoint_illumination}
% ===================================================================

Analyzing the breakdown of full-image results provides insight into where these methods excel:

\begin{enumerate}
    \item \textbf{Traditional Methods}: SIFT benefits immensely from scale control on viewpoint sequences (+19.8\% HP-V vs +16.7\% HP-I), confirming that scale variance is a primary source of error for hand-crafted detectors in geometric tasks.
    \item \textbf{Learned Methods}: HardNet and SOSNet are naturally robust to illumination changes (HP-I $>$ HP-V).
    \item \textbf{Fusion}: The fused CNN descriptor achieves excellent performance on both tasks (92.6\% HP-V, 94.2\% HP-I), effectively closing the gap between geometric and photometric invariance.
\end{enumerate}

% ===================================================================
\section{Color Patch Benchmark Results}%
\label{sec:patch_results}
% ===================================================================

The patch benchmark isolates descriptor effects by using pre-extracted 65$\times$65 color patches. All descriptors are computed on identical patch locations, removing keypoint quality as a confounding variable.

\subsection{Baseline Descriptor Performance on Patches}

\begin{table}[ht]
\centering
\caption{Single descriptor performance on color patch benchmark}%
\label{tab:patch_baselines}
\begin{tabular}{lrrrr}
\toprule
\textbf{Descriptor} & \textbf{Dim} & \textbf{Matching} & \textbf{Verification} & \textbf{Retrieval} \\
\midrule
SOSNet & 128 & 48.9\% & 89.5\% & 57.8\% \\
HardNet & 128 & 48.4\% & 89.3\% & 56.9\% \\
SIFT & 128 & 22.9\% & 65.5\% & 31.1\% \\
HoNC & 128 & 18.5\% & 71.0\% & 33.3\% \\
\bottomrule
\end{tabular}
\end{table}

CNN descriptors (HardNet, SOSNet) achieve the best single-descriptor matching performance. Notably, HoNC has low matching (18.5\%) but high verification (71.0\%), suggesting strong discriminative capability---it rejects false matches well but struggles with invariance to viewpoint/illumination changes.

\subsection{The Discriminator-Matcher Framework}

We characterize descriptors by their \textbf{verification-to-matching (V/M) ratio}:

\begin{table}[ht]
\centering
\caption{Descriptor ``personality'' based on V/M ratio}
\label{tab:vm_ratio}
\begin{tabular}{lrrrl}
\toprule
\textbf{Descriptor} & \textbf{Matching} & \textbf{Verification} & \textbf{V/M Ratio} & \textbf{Type} \\
\midrule
HoNC & 18.5\% & 71.0\% & 3.84$\times$ & Discriminator \\
SIFT & 22.9\% & 65.5\% & 2.86$\times$ & Balanced \\
HardNet & 48.4\% & 89.3\% & 1.84$\times$ & Matcher \\
\bottomrule
\end{tabular}
\end{table}

\begin{itemize}
    \item \textbf{High V/M ratio (Discriminator)}: Good at rejecting false matches, struggles with invariance
    \item \textbf{Low V/M ratio (Matcher)}: Trained for correspondence, naturally invariant
\end{itemize}

This framework predicts that pairing a discriminator with a matcher yields the best fusion results.

\subsection{Descriptor Fusion on Patches}

\begin{table}[ht]
\centering
\caption{Descriptor fusion results on color patch benchmark}
\label{tab:patch_fusion}
\begin{tabular}{llrrrr}
\toprule
\textbf{Fusion} & \textbf{Method} & \textbf{Dim} & \textbf{Matching} & \textbf{Verif.} & \textbf{Retrieval} \\
\midrule
\multicolumn{6}{l}{\textit{Complementary fusion (Discriminator + Matcher):}} \\
HoNC + SOSNet & Concat & 256 & \textbf{50.6\%} & 79.6\% & \textbf{61.4\%} \\
HoNC + HardNet & Concat & 256 & 50.1\% & 79.3\% & 60.5\% \\
HoNC + SOSNet & Average & 128 & 49.5\% & 79.1\% & 59.7\% \\
\midrule
\multicolumn{6}{l}{\textit{Same-family fusion (Matcher + Matcher):}} \\
HardNet + SOSNet & Concat & 256 & 49.9\% & \textbf{90.1\%} & 58.6\% \\
\midrule
\multicolumn{6}{l}{\textit{Failed fusion (Similar descriptors):}} \\
SIFT + HoNC & Concat & 256 & 15.5\% & --- & --- \\
SIFT + RGBSIFT & Concat & 512 & 10.4\% & --- & --- \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key Findings:}
\begin{enumerate}
    \item \textbf{Complementary fusion wins}: HoNC+SOSNet (50.6\%) outperforms HardNet+SOSNet (49.9\%) despite HoNC alone being much weaker (18.5\% vs 48.4\%). The color discriminator adds value that CNN matchers lack.

    \item \textbf{Similar descriptors fail}: SIFT+HoNC and SIFT+RGBSIFT perform \textit{worse} than either component alone. These descriptors capture correlated information (gradient histograms), so fusion provides no benefit.

    \item \textbf{Concatenation outperforms averaging}: Across all successful fusions, concatenation yields 1--2\% higher mAP than averaging by preserving complementary information.
\end{enumerate}

\subsection{Why HoNC Adds Value}

HoNC captures color information that gradient-based descriptors (SIFT, HardNet) discard:
\begin{itemize}
    \item CNN descriptors are trained on grayscale patches, learning edge and texture patterns
    \item HoNC captures chromatic distinctiveness (e.g., red vs. blue regions)
    \item The combination provides both color discrimination and geometric/photometric invariance
\end{itemize}

This explains why HoNC+CNN outperforms CNN+CNN: the descriptors capture genuinely different information.

\section{Summary}
% DRAFTING: Updated to clearly separate full-image and patch findings

Our experiments demonstrate four key conclusions, drawing on results from both evaluation pipelines:

\textbf{Full-Image Pipeline Findings (Detector Effects):}
\begin{enumerate}
    \item \textbf{Scale Matters}: Filtering for larger scales improves performance by 13--18\% across all descriptor types.
    \item \textbf{Detector Consensus Matters}: Spatial intersection of distinct detectors acts as a quality filter, boosting mAP by 4--18\%.
    \item \textbf{CNN+CNN Fusion Succeeds}: HardNet+SOSNet concatenation achieves 93.4\% mAP on intersection keypoints.
\end{enumerate}

\textbf{Patch Benchmark Findings (Descriptor Effects):}
\begin{enumerate}
    \setcounter{enumi}{3}
    \item \textbf{Complementary Fusion Excels}: Pairing color descriptors (HoNC) with learned descriptors (CNN) achieves best patch performance (50.6\%), outperforming CNN+CNN fusion (49.9\%) because color and learned features capture different information.
    \item \textbf{Magnitude Matching Required}: Cross-family fusion (SIFT+CNN) requires pre-fusion L2 normalization to ensure equal contribution from each descriptor.
\end{enumerate}
