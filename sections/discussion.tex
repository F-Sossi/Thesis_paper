% Chapter 5: Discussion
% DRAFTING: New discussion chapter 2025-12-07

This chapter interprets our experimental findings, explaining the underlying causes of observed performance patterns and providing practical recommendations.

\section{Why Scale Control Matters}

Our experiments demonstrate that scale control yields dramatic improvements: +39\% relative for SIFT-family descriptors and +21\% relative for CNN descriptors. We attribute this to several factors:

\subsection{Information Content}

Larger-scale keypoints capture patches containing more pixels:
\begin{itemize}
    \item A 4-pixel scale keypoint samples approximately a 16$\times$16 pixel region
    \item A 10-pixel scale keypoint samples approximately a 40$\times$40 pixel region
    \item Larger regions contain more distinctive texture and edge information
\end{itemize}

\subsection{Aliasing and Noise}

Small-scale keypoints are more susceptible to:
\begin{itemize}
    \item \textbf{Aliasing}: High-frequency content that violates Nyquist sampling
    \item \textbf{Noise sensitivity}: Small patches have lower signal-to-noise ratio
    \item \textbf{Localization error}: Sub-pixel errors have greater relative impact
\end{itemize}

\subsection{Practical Recommendation}

For production systems requiring high matching accuracy, we recommend filtering to the top 25\% of keypoints by scale. The trade-off is reduced keypoint count (645K vs 2.5M in our experiments), but the quality improvement substantially outweighs the coverage reduction.

\section{Understanding Fusion: Magnitude Matching}
% DRAFTING: Simplified to focus on technical requirement, not bug discovery

\subsection{Pre-Fusion Normalization Requirement}

Cross-family descriptor fusion (e.g., SIFT+CNN) requires careful attention to descriptor magnitudes. Different descriptor families produce values at different scales:

\begin{table}[h]
\centering
\caption{Descriptor magnitude characteristics}
\begin{tabular}{lcc}
\toprule
\textbf{Descriptor} & \textbf{Raw Range} & \textbf{After L2 Norm} \\
\midrule
SIFT & [0, 512] & [0, 0.3] \\
HardNet/SOSNet & [-0.3, +0.3] & [-0.3, +0.3] \\
HoNC & [0, 1] & [0, 0.3] \\
\bottomrule
\end{tabular}
\end{table}

Without normalization, larger-magnitude descriptors dominate L2 distance calculations. The solution is to L2 normalize each descriptor component \textit{before} fusion:

\begin{equation}
    d_{\text{fused}} = \text{fuse}\left(\frac{d_A}{\|d_A\|_2}, \frac{d_B}{\|d_B\|_2}\right)
\end{equation}

With pre-fusion normalization, SIFT+HardNet achieves 46.0\% mAP on the patch benchmark---comparable to other cross-family fusions.

\subsection{Why Some Fusion Still Underperforms}

Even with proper normalization, SIFT+CNN fusion (46.0\%) underperforms HoNC+CNN fusion (50.6\%) because:
\begin{enumerate}
    \item SIFT and CNN both capture gradient/edge information (correlated)
    \item HoNC captures color information that CNN lacks (complementary)
    \item Complementary descriptors provide more benefit than similar ones
\end{enumerate}

The key insight is that fusion success depends on \textit{complementarity}: descriptors that capture different information combine better than those capturing similar information.

\section{Why CNN+CNN Fusion Succeeds}

HardNet+SOSNet concatenation achieves 93.4\% mAP on full images, improving upon either descriptor alone. We attribute this success to:

\subsection{Magnitude Compatibility}
% DRAFTING: Renamed from "Distribution Compatibility" for consistency

Both descriptors are already L2-normalized during training:
\begin{itemize}
    \item Similar value ranges ([-0.3, +0.3])
    \item Unit L2 norm (no magnitude mismatch)
    \item No pre-fusion normalization needed
\end{itemize}

This ensures equal contribution from each descriptor without additional processing.

\subsection{Complementary Features}

Despite similar training methodologies, HardNet and SOSNet learn slightly different representations:
\begin{itemize}
    \item \textbf{HardNet}: Trained with hard negative mining, focuses on discriminative features
    \item \textbf{SOSNet}: Incorporates second-order similarity, captures different geometric relationships
\end{itemize}

Concatenation preserves both representations, allowing the matcher to utilize all available information.

\subsection{Why Concatenation Outperforms Averaging}

Concatenation (+0.5\%) outperforms averaging (-0.6\%) because:
\begin{enumerate}
    \item Averaging collapses 256 dimensions of information into 128
    \item Complementary features may be lost in averaging
    \item Concatenation preserves full information from both descriptors
\end{enumerate}

\section{Detector Agreement as Quality Signal}
% DRAFTING: Expanded based on intersection investigation findings

Keypoints detected by both SIFT and KeyNet (spatial intersection) achieve higher performance than either detector alone:
\begin{itemize}
    \item HardNet on full KeyNet: 64.5\% mAP
    \item HardNet on intersection: 82.1\% mAP (+17.6\%)
\end{itemize}

We conducted a systematic investigation to understand whether this improvement stems from the \textit{keypoints} being higher quality or the \textit{descriptors} computed on them being more distinctive.

\subsection{Investigation Methodology}

We tested five hypotheses for the intersection gain:
\begin{enumerate}
    \item \textbf{Reduced count effect}: Fewer keypoints reduce false matches
    \item \textbf{Quality selection}: Intersection selects stronger keypoints (higher response)
    \item \textbf{Scale selection}: Intersection favors certain scale ranges
    \item \textbf{Spatial distribution}: Intersection removes clustered keypoints
    \item \textbf{Detector agreement}: Keypoints where detectors agree are more repeatable
\end{enumerate}

We created control keypoint sets: random subsets, top-N by response, top-N by scale, and spatially filtered sets---all matched to the intersection count (645K keypoints).

\subsection{Key Finding: Keypoint Quality, Not Descriptor Distinctiveness}

The investigation revealed that the improvement comes from \textbf{keypoint quality}, not descriptor properties:

\begin{itemize}
    \item \textbf{Random subsets do not match intersection performance}: Simply reducing keypoint count does not explain the gain. Random subsets of equal size perform worse than intersection sets.

    \item \textbf{Scale filtering provides the largest single gain}: Filtering to large-scale keypoints (top 25\% by size) improved SIFT by 50\% (42.6\% $\rightarrow$ 63.9\%) and HardNet by 20\% (65.8\% $\rightarrow$ 78.9\%).

    \item \textbf{Intersection provides additional gain beyond scale}: HardNet on intersection (82.1\%) exceeds HardNet on scale-controlled sets (78.9\%), indicating intersection captures quality beyond scale alone.
\end{itemize}

\subsection{Why Detector Agreement Indicates Quality}

The intersection filters for keypoints that are \textit{genuinely salient image features} rather than detector-specific artifacts:

\begin{itemize}
    \item \textbf{Cross-validation}: If both a blob detector (KeyNet) and an edge/corner detector (SIFT) independently identify the same location as interesting, it likely corresponds to a real image structure.

    \item \textbf{Repeatability}: Keypoints detected by multiple methods are more likely to be detected again under viewpoint/illumination changes---they are inherently more repeatable.

    \item \textbf{Noise rejection}: Detector-specific noise or false detections are unlikely to occur at the same spatial location across different detection methods.
\end{itemize}

\subsection{Critical Distinction}

The improvement is not because descriptors computed on intersection keypoints are inherently more distinctive. The same HardNet model produces the same descriptor for a given image patch regardless of how the patch location was selected.

Rather, the improvement occurs because:
\begin{enumerate}
    \item Intersection keypoints correspond to more repeatable image structures
    \item These structures are more likely to be detected in both reference and target images
    \item Higher repeatability leads to more correct correspondences being available for matching
\end{enumerate}

This finding has practical implications: \textbf{keypoint selection strategy can matter as much as descriptor algorithm choice}. The 17\% gain from intersection filtering is comparable to the gap between SIFT and state-of-the-art CNN descriptors.

\section{HP-V vs HP-I Patterns}

\subsection{Traditional vs Learned Preferences}

Our results show opposite patterns:
\begin{itemize}
    \item \textbf{Traditional descriptors}: HP-V $>$ HP-I (better on viewpoint changes)
    \item \textbf{CNN descriptors}: HP-I $>$ HP-V (better on illumination changes)
\end{itemize}

\subsection{Explanation}

CNN descriptors are trained on patches with photometric augmentations (brightness, contrast, gamma), leading to learned illumination invariance. Traditional descriptors rely on gradient directions, which are naturally invariant to multiplicative illumination changes but not to more complex photometric transformations.

\section{Limitations}

\subsection{Dataset Scope}

Our experiments use only the HPatches benchmark. While HPatches is a standard evaluation dataset, results may not generalize to:
\begin{itemize}
    \item Extreme viewpoint changes ($>$ 60 degrees)
    \item Significant scale differences between images
    \item Different image domains (medical, satellite, etc.)
\end{itemize}

\subsection{Computational Considerations}

Concatenation doubles descriptor dimensionality (128D to 256D), increasing:
\begin{itemize}
    \item Memory requirements
    \item Matching time (linear in dimensionality for brute-force)
    \item Index size for approximate nearest neighbor methods
\end{itemize}

For real-time applications, this overhead may be prohibitive.

\subsection{Detector Dependency}

Our best results use KeyNet for CNN descriptors. The findings may not transfer to other detector choices (e.g., SuperPoint's built-in detector).

\section{Summary of Insights}
% DRAFTING: Updated to reflect keypoint quality findings from intersection investigation

\begin{enumerate}
    \item \textbf{Scale matters more than descriptor choice}: A 50\% relative improvement from scale control (SIFT: 42.6\% $\rightarrow$ 63.9\%) exceeds most algorithmic improvements

    \item \textbf{Keypoint selection matters as much as descriptor algorithm}: The 17\% gain from detector intersection is comparable to the gap between traditional and CNN descriptors

    \item \textbf{Detector agreement signals keypoint quality, not descriptor distinctiveness}: Intersection keypoints are more \textit{repeatable}---the descriptors themselves are not more distinctive, but the underlying image structures are more reliably detected across images

    \item \textbf{Complementary descriptors outperform similar ones}: HoNC+CNN fusion (50.6\% on patches) beats CNN+CNN (49.9\%) because color and learned features capture different information

    \item \textbf{Magnitude matching enables cross-family fusion}: Pre-fusion L2 normalization is required for SIFT+CNN fusion

    \item \textbf{Concatenation $>$ averaging}: Preserving full information outperforms lossy aggregation across all successful fusions
\end{enumerate}
