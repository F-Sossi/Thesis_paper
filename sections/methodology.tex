% Chapter 3: Methodology
% TODO: Remove DRAFTING comments before final submission

This chapter describes our methodology for cross-detector descriptor fusion, including the spatial intersection algorithm, scale-matching strategy, and evaluation framework.

\section{DescriptorWorkbench Framework}

All experiments were conducted on a workstation with the following specifications:

\begin{itemize}
    \item \textbf{CPU}: Intel Core i9-9900K @ 3.60GHz (8 cores, 16 threads)
    \item \textbf{RAM}: 94 GB DDR4
    \item \textbf{GPU}: NVIDIA GeForce RTX 4090 (24 GB VRAM)
    \item \textbf{OS}: Manjaro Linux (Kernel 6.12)
    \item \textbf{CUDA}: 13.1
    \item \textbf{OpenCV}: 4.13.0
    \item \textbf{LibTorch}: 2.10.0
\end{itemize}

We developed DescriptorWorkbench, an open-source evaluation framework for local feature descriptor research. It provides:

\begin{itemize}
    \item \textbf{Modular architecture}: Pluggable extractors, pooling, and matchers
    \item \textbf{Database storage}: SQLite-based experiment tracking with comprehensive metrics
    \item \textbf{YAML configuration}: Declarative experiment specification
    \item \textbf{CLI tools}: \texttt{experiment\_runner} for evaluation, \texttt{keypoint\_manager} for keypoint set operations
\end{itemize}

\subsection{Supported Descriptors}

The framework implements the following descriptor types, using OpenCV for traditional methods and LibTorch for deep learning models:

\begin{table}[ht]
\centering
\caption{Descriptor implementations in DescriptorWorkbench}
\label{tab:descriptors}
\begin{tabular}{llrl}
\toprule
\textbf{Type} & \textbf{Family} & \textbf{Dim} & \textbf{Backend} \\
\midrule
SIFT & Traditional & 128 & OpenCV \\
RootSIFT & Traditional & 128 & OpenCV + transform \\
DSP-SIFT v2 & Traditional & 128 & Custom (pyramid-aware) \\
RGBSIFT & Traditional & 384 & Custom (per-channel) \\
SURF & Traditional & 64/128 & OpenCV \\
HoNC & Color & 128 & Custom \\
HardNet & Learned & 128 & LibTorch (C++) \\
SOSNet & Learned & 128 & LibTorch (C++) \\
L2-Net~\cite{tian2017l2net} & Learned & 128 & LibTorch (C++) \\
Composite & Fusion & varies & Custom \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Database Schema}

Experiment results are stored in SQLite with tables organized into two groups:

\textbf{Full-Image Pipeline Tables:}
\begin{itemize}
    \item \texttt{experiments}: Descriptor type, dataset, pooling strategy, keypoint set reference, and execution parameters
    \item \texttt{results}: True mAP (macro/micro), HP-V/HP-I breakdown, verification AP, retrieval AP, and timing metrics
    \item \texttt{keypoint\_sets}: Named keypoint collections with generation method, intersection provenance, and statistics
    \item \texttt{locked\_keypoints}: Individual keypoint records with coordinates, scale, angle, response, and octave
    \item \texttt{keypoint\_detector\_attributes}: Per-detector attributes for intersection keypoints
    \item \texttt{descriptors}: Cached descriptor vectors with normalization and pooling metadata
\end{itemize}

\textbf{Patch Benchmark Tables:}
\begin{itemize}
    \item \texttt{patch\_benchmark\_results}: mAP by difficulty (easy/hard/tough) and scene type (HP-V/HP-I), verification and retrieval metrics
    \item \texttt{patch\_benchmark\_task\_sets}: Named evaluation task configurations
    \item \texttt{patch\_benchmark\_verification\_pairs}: Positive and negative patch pairs for verification
    \item \texttt{patch\_benchmark\_retrieval\_queries}: Query patches for retrieval evaluation
    \item \texttt{patch\_benchmark\_retrieval\_distractors}: Distractor patches for retrieval
    \item \texttt{patch\_benchmark\_descriptor\_sets}: Cached descriptor configurations with parameters
    \item \texttt{patch\_benchmark\_descriptors}: Pre-computed descriptor matrices stored as blobs
\end{itemize}

\subsection{Two Evaluation Pipelines}
% DRAFTING: Added to clarify which pipeline is used for which claims

We use two complementary evaluation pipelines to isolate different experimental variables:

\subsubsection{Full-Image Pipeline}

The full-image pipeline extracts descriptors from complete HPatches images at detected keypoint locations. This pipeline is used for:
\begin{itemize}
    \item \textbf{Detector fusion experiments}: Evaluating how keypoint set selection (intersection, scale filtering) affects performance
    \item \textbf{Keypoint quality studies}: Comparing full keypoint sets vs.\ filtered subsets
\end{itemize}

In this pipeline, both keypoint quality and descriptor quality affect results, making it suitable for studying detector effects but not for pure descriptor comparisons.

\subsubsection{Patch-Based Pipeline}

The patch-based pipeline extracts descriptors from pre-extracted image patches (65$\times$65 pixels). This pipeline is used for:
\begin{itemize}
    \item \textbf{Descriptor fusion experiments}: Comparing descriptors and fusion strategies with keypoint quality held constant
    \item \textbf{Color descriptor evaluation}: Testing HoNC and other color-aware descriptors
\end{itemize}

By using identical patch locations for all descriptors, this pipeline isolates descriptor effects from keypoint quality effects.

\subsubsection{Color HPatches Patch Benchmark}
% DRAFTING: Complete re-implementation documentation

The original HPatches patch benchmark provides gray scale 65$\times$65 patches, which prevents evaluation of color descriptors like HoNC. The original keypoint locations were not publicly released, so we developed \texttt{patch\_dataset\_builder} to reconstruct the benchmark methodology.

\textbf{Reconstruction Methodology:}
\begin{enumerate}
    \item \textbf{Keypoint Detection}: Run three detector types (Harris, Hessian/SURF, DoG/SIFT) on the original HPatches images, following the detector mix described in the original paper~\cite{balntas2017hpatches}. Keypoints are stored in the database with response-based ranking.
    \item \textbf{Jitter Application}: Apply the publicly available jitter parameters (rotation, translation, scale, anisotropy) from the original HPatches release. These per-patch geometric perturbations create the easy/hard/tough difficulty levels.
    \item \textbf{Patch Extraction}: For each keypoint, compute the patch region using scale $\times$ 5 (following HPatches conventions), apply jitter transformations, project through ground-truth homographies, and extract 65$\times$65 patches using perspective warping.
    \item \textbf{Overlap Filtering}: Cluster spatially overlapping patches (IoU $>$ 0.5) to avoid redundant samples, matching the original benchmark's diversity.
\end{enumerate}

\textbf{Validation:}
We validated the reconstruction by comparing overlap distributions and SIFT baseline performance against the original benchmark:

\begin{table}[ht]
\centering
\caption{Color HPatches validation against original benchmark}
\label{tab:hpatches_validation}
\begin{tabular}{lccc}
\toprule
\textbf{Metric} & \textbf{Original} & \textbf{Rebuilt} & \textbf{Difference} \\
\midrule
SIFT Matching mAP & 25.47\% & 22.9\% & --2.5\% \\
SIFT Verification AP & 65.12\% & 65.5\% & +0.4\% \\
SIFT Retrieval AP & 31.98\% & 31.1\% & --0.9\% \\
\bottomrule
\end{tabular}
\end{table}

The rebuilt benchmark produces slightly lower matching scores (22.9\% vs 25.47\%), indicating marginally harder patches, while verification and retrieval metrics are nearly identical. The reconstruction closely matches the original benchmark while adding color support.

\subsection{Deep Learning Integration}

Integrating deep learning models (typically implemented in Python/PyTorch) into a high-performance C++ evaluation pipeline was necessary to compare against optimized traditional descriptors like SIFT.

\subsubsection{Hybrid Architecture}

We use a hybrid pipeline that uses the best tool for each stage:

\begin{itemize}
    \item \textbf{Keypoint Detection (Python/Kornia)}: KeyNet detection is performed offline using Python scripts that interface with the Kornia library~\cite{riba2020survey}. This ensures 100\% fidelity to the reference implementation and avoids re-implementation errors. Coordinates are serialized to the SQLite database for reuse.
    \item \textbf{Descriptor Extraction (C++/LibTorch)}: For descriptor extraction, which must occur in the inner loop of matching experiments, we integrated the LibTorch (PyTorch C++) frontend directly into our application.
\end{itemize}

\subsubsection{Rejection of ONNX}

We initially attempted to deploy models using the ONNX (Open Neural Network Exchange) format via OpenCV's DNN module. This approach proved unsuitable:
\begin{itemize}
    \item \textbf{Operator Incompatibility}: Key models like HardNet use specific normalization layers (e.g., \texttt{InstanceNorm}) and control structures that were not fully supported by the OpenCV ONNX importer at the time of development.
    \item \textbf{Performance Stability}: We observed inconsistent behavior and occasional crashes with complex graphs exported from newer PyTorch versions.
\end{itemize}

We deprecated the ONNX pipeline in favor of TorchScript. Models are exported to the \texttt{.pt} format and loaded via \texttt{torch::jit::load} within our C++ \texttt{LibTorchWrapper}. This provides native PyTorch execution and numerical equivalence to the Python training code, while running inside the C++ benchmarking tool.

\subsection{Use of AI-Assisted Development Tools}

Development of DescriptorWorkbench used Claude~\cite{anthropic2025claude}, a large language model, as a coding assistant. The tool was used for:

\begin{itemize}
    \item \textbf{Code refactoring}: Restructuring existing C++ modules (e.g., migrating from monolithic functions to the modular extractor/factory architecture)
    \item \textbf{Code review}: Identifying bugs, suggesting improvements to error handling, and checking consistency across the YAML configuration and database schema
    \item \textbf{Editing}: Revising draft text in this document for clarity and grammar
\end{itemize}

All experimental design, data collection, analysis, and scientific conclusions are the author's own. The AI tool was not used to generate experimental data or interpret results.

\section{Spatial Intersection Algorithm}%
\label{sec:intersection_algorithm}

To enable cross-detector descriptor fusion, we need to establish correspondence between keypoints detected by different methods. We employ a mutual nearest neighbor (MNN) algorithm with spatial tolerance.

\subsection{Algorithm Description}

Given two keypoint sets $K_A$ (e.g., SIFT-detected) and $K_B$ (e.g., KeyNet-detected), we compute the intersection as follows:

\begin{enumerate}
    \item Build KD-tree spatial indices for both sets: $T_A$, $T_B$

    \item For each keypoint $k_a \in K_A$:
    \begin{enumerate}
        \item Find nearest neighbor $k_b = \text{NN}(k_a, T_B)$
        \item Check forward tolerance: $\|k_a - k_b\|_2 \leq \tau$
        \item Find reverse nearest neighbor $k_a' = \text{NN}(k_b, T_A)$
        \item Check mutual agreement: $k_a' = k_a$
        \item Check reverse tolerance: $\|k_b - k_a'\|_2 \leq \tau$
        \item Check uniqueness: $k_b$ not already matched
        \item If all checks pass, add $(k_a, k_b)$ to intersection
    \end{enumerate}

    \textbf{Note on the reverse tolerance check:} When mutual agreement holds ($k_a' = k_a$), the reverse tolerance check (e) is algebraically identical to the forward check (b), since $\|k_b - k_a'\|_2 = \|k_b - k_a\|_2$. The reverse check only provides additional filtering when mutual agreement fails ($k_a' \neq k_a$), in which case the pair is already rejected by check (d). In practice, check (e) is therefore redundant given check (d), but we retain it for defensive correctness.

    \textbf{Note on KD-tree usage:} For 2D spatial matching, a KD-tree is not strictly necessary---a brute-force search over $(x, y)$ coordinates would suffice for our dataset sizes. We use a KD-tree because OpenCV's \texttt{cv::flann} provides an efficient, well-tested implementation, and the data structure generalizes naturally if future work extends the matching criteria to higher-dimensional spaces (e.g., incorporating scale or orientation).

    \item Output: Paired keypoint sets $(K_A^*, K_B^*)$ with 1-to-1 correspondence
\end{enumerate}

\subsection{Algorithm Properties}

The MNN algorithm guarantees several properties:

\begin{itemize}
    \item \textbf{1-to-1 correspondence}: Each keypoint matched at most once
    \item \textbf{Symmetry}: Same result regardless of which set is processed first
    \item \textbf{Tolerance-based}: Configurable spatial acceptance threshold
    \item \textbf{Mutual agreement}: Both keypoints must be each other's nearest neighbor
\end{itemize}

% ============================================================================
% DRAFTING: Clarification that intersection only uses (x,y), not scale/orientation
% ============================================================================
\subsubsection{Matching Criteria Limitations}

The current implementation uses \textit{only spatial position} $(x, y)$ for keypoint matching. Scale (\texttt{cv::KeyPoint::size}) and orientation (\texttt{cv::KeyPoint::angle}) are \textit{not} used as matching criteria:

\begin{verbatim}
// Only x,y coordinates populate the KD-tree
data.at<float>(i, 0) = keypoint.pt.x;
data.at<float>(i, 1) = keypoint.pt.y;
// keypoint.size and keypoint.angle are NOT included
\end{verbatim}

The matched keypoints retain their original scale and orientation properties (stored in the database), but these attributes do not influence which keypoints are paired only spatial proximity determines correspondence.

\textbf{Rationale:} Different detectors produce vastly different scale estimates for the same image location (e.g., SIFT averages 4.45px while KeyNet averages 49.83px on HPatches). Requiring scale agreement would eliminate most correspondences. Similarly, orientation estimates vary significantly between detector families.

\textbf{Future work:} Scale-aware matching could be explored by normalizing scales relative to each detector's distribution before matching, or by using a scale-ratio threshold in addition to spatial tolerance.
% ============================================================================
% END DRAFTING
% ============================================================================

\subsection{Tolerance Selection}

Following Mikolajczyk and Schmid's detector evaluation methodology, we use a default tolerance of $\tau = 3.0$ pixels~\cite{mikolajczyk2003evaluation}.
This value balances:

\begin{itemize}
    \item \textbf{Strict enough}: Prevents misaligned correspondences
    \item \textbf{Loose enough}: Accounts for typical detector variance (1-3 pixels)
    \item \textbf{Literature-supported}: Standard practice in feature matching research
\end{itemize}

% ============================================================================
% DRAFTING: Clarification that tolerance is NOT ground truth tolerance
% ============================================================================
\textbf{Important distinction:} The tolerance parameter $\tau$ used in intersection computation is \textit{not} related to ground truth matching error tolerance. It defines the maximum spatial distance (in pixels) between keypoint centers from different detectors to consider them as detecting the ``same'' image feature. This is a detector-level correspondence criterion, not an evaluation metric.

For example, if SIFT detects a corner at $(100.0, 200.0)$ and KeyNet detects a blob at $(102.1, 201.5)$, the Euclidean distance is 2.65 pixels. With $\tau = 3.0$, these keypoints are paired as corresponding to the same image location.

\textbf{Tolerance vs. scale:} The current implementation uses a fixed pixel tolerance regardless of keypoint scale. A scale-proportional tolerance (e.g., $\tau = 0.3 \times \text{avg\_scale}$) could be explored in future work, though the fixed tolerance has proven effective and computationally efficient.

\textbf{Experimental validation:} We tested tolerance values from 1.0 to 10.0 pixels using DSP-SIFT on SIFT--KeyNet intersection sets:
\begin{itemize}
    \item $\tau = 1.0$ (strict): 67.41\% mAP---fewest keypoints, highest alignment quality
    \item $\tau = 2.0$: 62.10\% mAP---moderate keypoint count
    \item $\tau = 5.0$ (relaxed): 57.31\% mAP---more keypoints, lower alignment quality
    \item $\tau = 10.0$ (very relaxed): 57.06\% mAP---diminishing returns beyond 5 pixels
\end{itemize}
Stricter tolerances yield higher mAP because they enforce tighter spatial alignment, producing more precisely corresponding patches at the cost of reduced keypoint count. We use $\tau = 3.0$ pixels as the default, following Mikolajczyk and Schmid~\cite{mikolajczyk2003evaluation}, which balances alignment quality against coverage. For scale-matched intersection sets (Section~\ref{sec:scale_matching}), we use a 6-pixel tolerance because larger-scale keypoints have inherently more localization variance.
% ============================================================================
% END DRAFTING
% ============================================================================

\subsection{Implementation}

The intersection algorithm is implemented in the \texttt{keypoint\_manager} CLI tool:

\begin{verbatim}
./keypoint_manager build-intersection \
    --source-a sift_8000 \
    --source-b keynet_8000 \
    --out-a sift_intersection \
    --out-b keynet_intersection \
    --tolerance 3.0
\end{verbatim}

The resulting keypoint sets are stored in the database with provenance tracking.

\section{Scale-Matching Strategy}
\label{sec:scale_matching}

We observed that SIFT and KeyNet detectors produce keypoints with different scale distributions. This motivated our scale-matching strategy.

\subsection{Scale Distribution Analysis}

Table~\ref{tab:scale_distribution} shows the scale characteristics of different keypoint sets:

\begin{table}[ht]
\centering
\caption{Scale distribution of keypoint sets}
\label{tab:scale_distribution}
\begin{tabular}{lrrr}
\toprule
\textbf{Keypoint Set} & \textbf{Count} & \textbf{Mean Scale} & \textbf{Std Dev} \\
\midrule
sift\_8000 (full) & 2.5M & 4.45px & 3.2px \\
keynet\_8000 (full) & 2.8M & 49.83px & 28.1px \\
sift\_scale\_6px (filtered) & 645K & 10.03px & 4.1px \\
keynet\_scale\_6px (filtered) & 645K & 92.39px & 31.2px \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Scale Filtering Methodology}

To create scale-controlled keypoint sets, we:

\begin{enumerate}
    \item Sort keypoints by scale (descending)
    \item Retain top $k$\% (typically 25\%)
    \item Apply minimum scale threshold (6 pixels) to exclude aliased features
\end{enumerate}

% ============================================================================
% DRAFTING: Added implementation details for professor's question on scale filtering
% ============================================================================
\subsubsection{Implementation Details}

The scale filtering is implemented by sorting keypoints by their \texttt{cv::KeyPoint::size} attribute, which represents the diameter of the detected feature in pixels. The implementation selects the top $N$ keypoints after sorting in descending order:

\begin{verbatim}
// Sort by keypoint size (scale) descending
std::sort(keypoints.begin(), keypoints.end(),
    [](const cv::KeyPoint& a, const cv::KeyPoint& b) {
        return a.size > b.size;
    });
// Keep only top N keypoints
keypoints.resize(target_count);
\end{verbatim}

\textbf{Why larger scales improve performance:} SIFT computes descriptors using $16 \times 16$ spatial bins at the keypoint's detected scale. Larger-scale keypoints capture more image context within each bin, producing more distinctive gradient histograms that are stable under geometric and photometric transformations. Small-scale keypoints ($<$6 pixels) suffer from aliasing and provide insufficient context for reliable matching.

\textbf{Experimental validation:} Scale filtering provides a +21\% absolute improvement for SIFT (42.64\% $\rightarrow$ 63.86\% mAP), representing a 50\% relative gain. This single optimization has greater impact than any fusion strategy tested.
% ============================================================================
% END DRAFTING
% ============================================================================

\subsection{Scale-Matched Intersection}

For cross-detector fusion, we combine scale filtering with spatial intersection:

\begin{enumerate}
    \item Generate scale-controlled SIFT keypoints: $K_A^{\text{scale}}$
    \item Generate scale-controlled KeyNet keypoints: $K_B^{\text{scale}}$
    \item Compute spatial intersection: $(K_A^*, K_B^*)$
\end{enumerate}

This produces aligned keypoint pairs where:
\begin{itemize}
    \item SIFT keypoints: Average 7.64 pixel scale
    \item KeyNet keypoints: Average 89.74 pixel scale
    \item Both sets: 111K paired keypoints with 1-to-1 correspondence
\end{itemize}

\section{Descriptor Fusion Methods}

With aligned keypoint sets, we implement two fusion strategies:

\subsection{Weighted Averaging}

\begin{equation}
    d_{\text{avg}} = \alpha \cdot d_A + (1 - \alpha) \cdot d_B
\end{equation}

where $\alpha \in [0, 1]$ controls the contribution of each descriptor. We use $\alpha = 0.5$ for equal weighting.

\textbf{Requirements}:
\begin{itemize}
    \item Same dimensionality: $\text{dim}(d_A) = \text{dim}(d_B)$
    \item Normalization: Both descriptors should be L2-normalized before averaging
\end{itemize}

\subsection{Concatenation}

\begin{equation}
    d_{\text{concat}} = [d_A, d_B]
\end{equation}

\textbf{Properties}:
\begin{itemize}
    \item Preserves both representations fully
    \item Doubles dimensionality: 128D + 128D = 256D
    \item No information loss from aggregation
\end{itemize}

\section{Experimental Pipeline}

Our experimental workflow integrates the components described above into a single pipeline:

\begin{enumerate}
    \item \textbf{Detection}: Generate keypoints for all HPatches images using detectors (SIFT, KeyNet) via Python/Kornia scripts.
    \item \textbf{Intersection}: Compute spatially aligned keypoint subsets using the MNN algorithm ($\tau=3.0$px) and scale filtering.
    \item \textbf{Extraction}: Compute descriptors for these locked keypoints. SIFT/RootSIFT use OpenCV; HardNet/SOSNet use the LibTorch C++ wrapper.
    \item \textbf{Fusion}: (Optional) Combine descriptors via concatenation or averaging.
    \item \textbf{Matching \& Evaluation}: Perform retrieval and matching tasks, computing metrics against ground-truth homographies.
\end{enumerate}

\section{Evaluation Methodology}

We use an evaluation strategy based on the protocols defined by Balntas et al.~\cite{balntas2017hpatches} and expanded by Bojani\'{c} et al.~\cite{bojanic2020comparison}.

\subsection{Task 1: Image Matching (mAP)}

The primary metric for descriptor utility is Mean Average Precision (mAP) in an image matching context.

\textbf{Protocol}:
\begin{enumerate}
    \item For each image pair (reference $I_A$, target $I_B$):
    \item Match descriptors using the Second Nearest Neighbor (SNN) ratio test.
    \item A match $(d_A^i, d_B^j)$ is correct if the geometric projection error $||H \cdot k_A^i - k_B^j||_2 \leq \tau$.
    \item Compute Average Precision (AP) as the area under the Precision-Recall curve.
\end{enumerate}

We report \textbf{True Macro mAP}: An Information Retrieval (IR) style metric where we compute AP per scene and average across scenes, ensuring that texture-poor scenes (with fewer keypoints) contribute equally. Crucially, we enforce a \textbf{Single Ground Truth (R=1)} policy: for a given keypoint in $I_A$, there is at most one correct match in $I_B$ (the nearest neighbor in the intersection set).

% ============================================================================
% DRAFTING: Detailed explanation of True mAP vs legacy precision
% ============================================================================
\subsubsection{True mAP vs. Legacy Precision}

We distinguish our ``True mAP'' metric from simpler precision-based measures sometimes reported in the literature:

\textbf{Legacy Precision (not used):}
\begin{itemize}
    \item Simple arithmetic mean of per-image precision values
    \item Formula: $\frac{1}{N}\sum_{i=1}^{N} \text{precision}_i$
    \item Does not account for ranking quality
\end{itemize}

\textbf{True mAP (our metric):}
\begin{itemize}
    \item Standard Information Retrieval Mean Average Precision
    \item For each query keypoint, compute the rank of the true match
    \item $\text{AP} = \frac{1}{\text{rank}}$ for single ground truth (R=1 policy)
    \item Average over all valid queries: $\text{mAP} = \frac{1}{|Q|}\sum_{q \in Q} \text{AP}_q$
\end{itemize}

\textbf{Implementation:} Ground truth is established by projecting query keypoints via the known homography matrix $H$ and finding the nearest target keypoint within a 3-pixel tolerance:
\begin{verbatim}
// Project query keypoint to target image
cv::Point2f projected = applyHomography(query_pt, H);
// Find nearest target keypoint within tolerance
int gt_idx = findNearest(projected, target_keypoints, tau=3.0);
// Compute rank: how many incorrect matches score better?
int rank = 1 + count_better_scoring_matches;
// AP = 1/rank for single ground truth
double ap = 1.0 / rank;
\end{verbatim}

This IR-style evaluation matches the standard used in HPatches benchmarks.
% ============================================================================
% END DRAFTING
% ============================================================================

\subsection{Task 2: Keypoint Verification}

Following Bojani\'{c} et al.~\cite{bojanic2020comparison}, verification tests the descriptor's ability to distinguish true correspondences from false ones.

\textbf{Binary Classification}:
\begin{itemize}
    \item \textbf{Positive pairs}: Spatially corresponding keypoints from sequence pairs.
    \item \textbf{Negative pairs}: Spatially non-corresponding keypoints (distractors).
\end{itemize}
We compute the Area Under the Receiver Operating Characteristic (ROC) Curve. The ROC curve plots the True Positive Rate (TPR) against the False Positive Rate (FPR) as the distance threshold varies from strict to permissive. A perfect descriptor achieves AUC = 1.0 (all positives ranked before all negatives), while a random baseline achieves AUC = 0.5. This threshold-independent metric measures discriminative power without committing to a specific matching strategy.

\subsection{Task 3: Keypoint Retrieval}

This task evaluates the descriptor's ranking capability. For a query patch, the system must rank a database of target patches.

\textbf{Labels}:
\begin{itemize}
    \item $y=1$: The true geometric match.
    \item $y=0$: "Hard negatives" (patches from the same image but different locations).
    \item $y=-1$: "Easy negatives" (patches from different sequences/scenes).
\end{itemize}
This metric tests whether the descriptor can rank true matches above hard negatives---patches from different locations that may look similar.

\subsection{Aggregation and Breakdown}

To account for the diversity of the HPatches dataset, we report results in two aggregation modes:
\begin{itemize}
    \item \textbf{Micro Average}: Aggregates all queries globally. Biases towards scenes with more keypoints.
    \item \textbf{Macro Average}: Computes metrics per scene, then averages across scenes. This ensures that texture-poor scenes (with fewer keypoints) contribute equally to the final score.
\end{itemize}

Results are further stratified by scene type:
\begin{itemize}
    \item \textbf{HP-V (Viewpoint)}: 59 sequences with significant geometric deformations.
    \item \textbf{HP-I (Illumination)}: 57 sequences with lighting changes (day/night, flash/no-flash).
\end{itemize}

\section{Experimental Design}

\subsection{Independent Variables}

Our experiments vary the following factors:

\begin{itemize}
    \item \textbf{Descriptor type}: SIFT, RootSIFT, DSP-SIFT, HardNet, SOSNet, HoNC
    \item \textbf{Keypoint set}: Full, scale-controlled, intersection, scale-matched intersection
    \item \textbf{Fusion method}: None, averaging, concatenation
    \item \textbf{Fusion pairs}: SIFT+CNN, CNN+CNN
\end{itemize}

\subsection{Dependent Variables}

We measure:
\begin{itemize}
    \item Mean Average Precision (mAP) - primary metric
    \item HP-V and HP-I breakdown
    \item Keypoint verification AP (when enabled)
    \item Keypoint retrieval AP (when enabled)
\end{itemize}

\subsection{Experiment Execution}

Experiments are specified in YAML configuration files and executed via:

\begin{verbatim}
./experiment_runner config/experiments/experiment.yaml
\end{verbatim}

Results are automatically stored in the SQLite database with full parameter logging for reproducibility.

\subsection{Computational Cost}

Table~\ref{tab:timing} reports wall-clock times for the full HPatches benchmark (116 sequences, 5 image pairs each) on the hardware described in Section~\ref{chap:methodology}.

\begin{table}[ht]
\centering
\caption{Processing time for full HPatches evaluation pipeline}
\label{tab:timing}
\begin{tabular}{lrrrr}
\toprule
\textbf{Configuration} & \textbf{Keypoints} & \textbf{Desc. (s)} & \textbf{Match (s)} & \textbf{Total (s)} \\
\midrule
\multicolumn{5}{l}{\textit{Full keypoint sets ($\sim$2.5M keypoints):}} \\
SIFT & $\sim$2.5M & 130 & 83 & 213 \\
DSP-SIFT & $\sim$2.5M & 657 & 102 & 759 \\
HardNet & $\sim$2.8M & 85 & 134 & 219 \\
SOSNet & $\sim$2.8M & 62 & 88 & 150 \\
\midrule
\multicolumn{5}{l}{\textit{Scale-controlled sets ($\sim$645K keypoints):}} \\
SIFT & $\sim$645K & 42 & 14 & 56 \\
HardNet & $\sim$645K & 25 & 14 & 38 \\
\midrule
\multicolumn{5}{l}{\textit{Scale-matched intersection ($\sim$111K keypoints):}} \\
HardNet+SOSNet (concat) & $\sim$111K & 7.2 & 0.5 & 7.7 \\
HardNet+SOSNet (avg) & $\sim$111K & 7.4 & 0.3 & 7.6 \\
\bottomrule
\end{tabular}
\end{table}

Keypoint filtering sharply reduces computation: scale-controlled sets run $4\times$ faster than full sets, and intersection sets run $28\times$ faster. Matching time scales quadratically with keypoint count (brute-force), so the intersection pipeline completes in under 8 seconds for the entire benchmark. Descriptor extraction dominates computation for DSP methods due to multi-scale processing.
